{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8468ae0-f92d-483a-a45c-4c2a33f87cd3",
   "metadata": {},
   "source": [
    "# Potential Challenges When Using LLMs\n",
    "- not natural language\n",
    "- sports is generated language\n",
    "- talk about SportsSett and its implications\n",
    "- this means __\n",
    "- because of this it's hard for LLMs to\n",
    "\n",
    "- Potential Challenges to Overcome\n",
    "\n",
    "\"The \"Hallucination\" Problem: LLMs may generate inaccurate information. Verification against trusted data sources and using LLMs for specific tasks (summaries, comparisons) rather than holistic analysis is crucial.\n",
    "Data Quality Limitations: The amount of well-structured and tagged basketball footage, especially at lower levels, might present a challenge for video analysis.\n",
    "Subjectivity: Areas like team fit and player character are hard for LLMs to grasp. The human element remains essential.\"\n",
    "\n",
    "- however, LLMs are good at this\n",
    "- potential use 1\n",
    "- potential use 2\n",
    "- potential use 3\n",
    "\n",
    "- really important to know what LLMs can and can not do at the moement\n",
    "- talk about the trasnformer network -- cite the MIT thign\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6ed82-a03b-4e4a-bfc6-32e68f5be608",
   "metadata": {
    "citation-manager": {
     "citations": {
      "kuchd": [
       {
        "id": "17220151/A7PNLSS5",
        "source": "zotero"
       }
      ],
      "nwj4g": [
       {
        "id": "17220151/A7PNLSS5",
        "source": "zotero"
       }
      ],
      "o333t": [
       {
        "id": "17220151/UWYAU6PB",
        "source": "zotero"
       }
      ],
      "stqoj": [
       {
        "id": "17220151/UWYAU6PB",
        "source": "zotero"
       }
      ],
      "vnj2p": [
       {
        "id": "17220151/UWYAU6PB",
        "source": "zotero"
       }
      ],
      "wg2p9": [
       {
        "id": "17220151/UWYAU6PB",
        "source": "zotero"
       }
      ],
      "zg6t9": [
       {
        "id": "17220151/A7PNLSS5",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "# Advancements in Sports Analytics and LLMs\n",
    "\n",
    "### SportQA: A Benchmark to Improve LLM Understanding of Sports\n",
    "SportQA aims to help LLMs gain a better understanding in sports knowledge. Through their research, they hope to allow LLMs to answer more complicated problems about sports rather than simply fact-based questions<cite id=\"nwj4g\"><a href=\"#zotero%7C17220151%2FA7PNLSS5\">(Xia et al., 2024)</a></cite>. Specifically, SportsQA is a large dataset that was created to test an LLM's knowledge of sports by often shuffling players from various sports and plays from different sports to see if the LLM can really understand sports<cite id=\"zg6t9\"><a href=\"#zotero%7C17220151%2FA7PNLSS5\">(Xia et al., 2024)</a></cite>.\n",
    "\n",
    "In the dataset, there are three different types of questions. The first is \"foundational knowledge\" which assesses the LLM's basic understanding of sports and focuses more on fact-based questions. The second type is \"rules and tactics comprehension,\" which involves asking the LLM to understand sports rules and strategies. The last type is \"advanced scenario-based understanding,\" these questions ask the LLM to analyze real sports situations<cite id=\"kuchd\"><a href=\"#zotero%7C17220151%2FA7PNLSS5\">(Xia et al., 2024)</a></cite>. \n",
    "\n",
    "By asking the LLMs these questions, SportsQA hopes to establish a benchmark for testing an LLM's understanding of sports. Overall, their founds showed that the models show promise for understanding facts, rules, and foundational knowledge in sports, but their skills in complicated sports questions are still very limited. Their findings show the need for greater research in Natural Language Processing and Artificial Intelligence to allow LLMs to have a better understanding of sports. \n",
    "\n",
    "### SportsSett: A Solution to Sports and Natural Language Generation\n",
    "It seems that there are some improvements being made to the interactions of LLMs with basketball and sports analytics. Specifically, they're aiming to fix the issues associated with Natural Language Generation (NLG) and creating basketball game summaries<cite id=\"wg2p9\"><a href=\"#zotero%7C17220151%2FUWYAU6PB\">(Thomson et al., 2020)</a></cite>.\n",
    "\n",
    "The paper points to the current resources available and how these datasets lack the data needed for generating summaries using advanced data analytics. A useful resource that exists is called the \"Rotowire and Rotowire-FG datasets.<cite id=\"stqoj\"><a href=\"#zotero%7C17220151%2FUWYAU6PB\">(Thomson et al., 2020)</a></cite>\" These datasets contain game summaries, box scores, and statistics to supplement the game summaries. While they contain a large amount of data, there are some limitations to these datasets <cite id=\"o333t\"><a href=\"#zotero%7C17220151%2FUWYAU6PB\">(Thomson et al., 2020)</a></cite>.\n",
    "\n",
    "However, there are many issues with these two datasets. The proposed solution is a new dataset called \"SportsSett.\" This dataset stores game statistics in a hierarchal structure, which allows the data to be queried from multiple dimensions. This structure will help allow for a richer \"entity-relationship graph <cite id=\"vnj2p\"><a href=\"#zotero%7C17220151%2FUWYAU6PB\">(Thomson et al., 2020)</a></cite>.\"\n",
    "\n",
    "### Existing Issues with Rotowire and Rotowire-FG\n",
    "The researchers who created SportsSett explained that there are issues with the existing Rotowire and Rotowire-FG datasets in terms of the data structure, availability, and length. In addition, the dataset lacks data on playoff games and preseason games. SportsSett seeks to solve these issues through data cleaning and redesign of data.\n",
    "\n",
    "The SportsSett database hopes to also expand upon the data by adding information such as stadium name and location. This is because, in basketball, home-court advantage and constant traveling can have very real effects on NBA teams. \n",
    "\n",
    "### Future Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89667b22-565e-4eaf-af61-f98f8e486966",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c160b-8c21-448f-ab05-8c3b8a757189",
   "metadata": {
    "citation-manager": {
     "citations": {
      "9kn5i": [],
      "hj7ur": [],
      "jfsfh": []
     }
    }
   },
   "source": [
    "At the moment LLMs struggle to understand sports and although LLMs can currently answer fact-based questions. When dealing with more rule-based, scenario, or expert level questions that involve a true knowledge of sports, these LLMs really struggle <cite id=\"hj7ur\"><a href=\"#zotero%7C17220151%2FA7PNLSS5\">(Xia et al., 2024)</a></cite>.\n",
    "\n",
    "This is mainly because sports is not natural language. There are many terms and nuances when discussing sports, that someone needs to understand in order to really talk about it and understand what they are talking about. Due to the nature of LLMs as \"next-word predictors,\" it seems confident and able to talk about sports <cite id=\"jfsfh\"><a href=\"#zotero%7C17220151%2FF8Z32QVX\">(<i>2. Introduction to Large Language Models for Text Generation</i>, n.d.)</a></cite>. But in reality, LLMs at the moment struggle to really understand sports. \n",
    "\n",
    "Often, even with fact-based questions, LLMs can hallucinate and provide false answers. This output can be dangerous for someone who doesn't know enough about the sport they are prompting about to know that the output is false. This is due to a mix of factors. The main ones being that in general, AI hallucinates and the other being that most LLMs such as ChatGPT 3.5, are not up to date<cite id=\"9kn5i\"><a href=\"#zotero%7C17220151%2FIWZG4XZB\">(<i>1. The Five Principles of Prompting</i>, n.d.)</a></cite>. Thus, asking the model questions about a basketball game that happened last night, will often yield false answers. \n",
    "\n",
    "Overall, it is very important that we understand what LLMs can and can not do at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469291d-e685-49cf-a169-208855939021",
   "metadata": {},
   "source": [
    "# Sources\n",
    "<!-- BIBLIOGRAPHY START -->\n",
    "<div class=\"csl-bib-body\">\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|17220151/UWYAU6PB\"></i>Thomson, C., Reiter, E., &#38; Sripada, S. (2020). SportSett:Basketball - A robust and maintainable data-set for Natural Language Generation. In D. Sánchez, R. Hervás, &#38; A. Gatt (Eds.), <i>Proceedings of the Workshop on Intelligent Information Processing and Natural Language Generation</i> (pp. 32–40). Association for Computational Lingustics. <a href=\"https://aclanthology.org/2020.intellang-1.4\">https://aclanthology.org/2020.intellang-1.4</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|17220151/A7PNLSS5\"></i>Xia, H., Yang, Z., Wang, Y., Tracy, R., Zhao, Y., Huang, D., Chen, Z., Zhu, Y., Wang, Y., &#38; Shen, W. (2024). <i>SportQA: A Benchmark for Sports Understanding in Large Language Models</i> (arXiv:2402.15862). arXiv. <a href=\"http://arxiv.org/abs/2402.15862\">http://arxiv.org/abs/2402.15862</a></div>\n",
    "</div>\n",
    "<!-- BIBLIOGRAPHY END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44b4fc-efbd-4add-9cf8-85aee62ce34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {
    "zotero": {
     "17220151/A7PNLSS5": {
      "URL": "http://arxiv.org/abs/2402.15862",
      "abstract": "A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs.",
      "accessed": {
       "date-parts": [
        [
         2024,
         4,
         4
        ]
       ]
      },
      "author": [
       {
        "family": "Xia",
        "given": "Haotian"
       },
       {
        "family": "Yang",
        "given": "Zhengbang"
       },
       {
        "family": "Wang",
        "given": "Yuqing"
       },
       {
        "family": "Tracy",
        "given": "Rhys"
       },
       {
        "family": "Zhao",
        "given": "Yun"
       },
       {
        "family": "Huang",
        "given": "Dongdong"
       },
       {
        "family": "Chen",
        "given": "Zezhi"
       },
       {
        "family": "Zhu",
        "given": "Yan"
       },
       {
        "family": "Wang",
        "given": "Yuan-fang"
       },
       {
        "family": "Shen",
        "given": "Weining"
       }
      ],
      "id": "17220151/A7PNLSS5",
      "issued": {
       "date-parts": [
        [
         2024,
         2,
         24
        ]
       ]
      },
      "note": "arXiv:2402.15862 [cs]",
      "number": "arXiv:2402.15862",
      "publisher": "arXiv",
      "shortTitle": "SportQA",
      "system_id": "zotero|17220151/A7PNLSS5",
      "title": "SportQA: A Benchmark for Sports Understanding in Large Language Models",
      "type": "article"
     },
     "17220151/UWYAU6PB": {
      "URL": "https://aclanthology.org/2020.intellang-1.4",
      "accessed": {
       "date-parts": [
        [
         2024,
         5,
         8
        ]
       ]
      },
      "author": [
       {
        "family": "Thomson",
        "given": "Craig"
       },
       {
        "family": "Reiter",
        "given": "Ehud"
       },
       {
        "family": "Sripada",
        "given": "Somayajulu"
       }
      ],
      "container-title": "Proceedings of the Workshop on Intelligent Information Processing and Natural Language Generation",
      "editor": [
       {
        "family": "Sánchez",
        "given": "Daniel"
       },
       {
        "family": "Hervás",
        "given": "Raquel"
       },
       {
        "family": "Gatt",
        "given": "Albert"
       }
      ],
      "event": "IntelLanG 2020",
      "event-place": "Santiago de Compostela, Spain",
      "id": "17220151/UWYAU6PB",
      "issued": {
       "date-parts": [
        [
         2020,
         9
        ]
       ]
      },
      "page": "32–40",
      "publisher": "Association for Computational Lingustics",
      "publisher-place": "Santiago de Compostela, Spain",
      "shortTitle": "SportSett",
      "system_id": "zotero|17220151/UWYAU6PB",
      "title": "SportSett:Basketball - A robust and maintainable data-set for Natural Language Generation",
      "type": "paper-conference"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3.11 (COMM4190)",
   "language": "python",
   "name": "comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
